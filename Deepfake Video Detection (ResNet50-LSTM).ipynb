{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2022-05-26T12:59:59.412932Z","iopub.execute_input":"2022-05-26T12:59:59.413224Z","iopub.status.idle":"2022-05-26T12:59:59.931403Z","shell.execute_reply.started":"2022-05-26T12:59:59.413169Z","shell.execute_reply":"2022-05-26T12:59:59.930715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install face_recognition","metadata":{"id":"7hlPaQS4e5VI","execution":{"iopub.status.busy":"2022-05-29T06:12:22.862569Z","iopub.execute_input":"2022-05-29T06:12:22.862934Z","iopub.status.idle":"2022-05-29T06:12:28.164124Z","shell.execute_reply.started":"2022-05-29T06:12:22.862874Z","shell.execute_reply":"2022-05-29T06:12:28.163330Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Requirement already satisfied: face_recognition in /opt/conda/lib/python3.6/site-packages (1.3.0)\nRequirement already satisfied: dlib>=19.7 in /opt/conda/lib/python3.6/site-packages (from face_recognition) (19.24.0)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.6/site-packages (from face_recognition) (5.4.1)\nRequirement already satisfied: Click>=6.0 in /opt/conda/lib/python3.6/site-packages (from face_recognition) (7.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from face_recognition) (1.17.4)\nRequirement already satisfied: face-recognition-models>=0.3.0 in /opt/conda/lib/python3.6/site-packages (from face_recognition) (0.3.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn import preprocessing\n\nlabel_data = pd.read_csv(\"../input/deepfake-dataset/New_DF.csv\")\nlabel_data.drop(axis=1, columns=['Unnamed: 0'], inplace=True)\npath = '../input/deepfake-dataset/Modified Dataset/Modified Dataset/'\nlabel_data['video_path'] = path+label_data['video']\n\nle = preprocessing.LabelEncoder()\nlabels = le.fit_transform(label_data['label'])\nlabel_data['labels'] = labels","metadata":{"execution":{"iopub.status.busy":"2022-05-26T13:11:26.670973Z","iopub.execute_input":"2022-05-26T13:11:26.671295Z","iopub.status.idle":"2022-05-26T13:11:28.02312Z","shell.execute_reply.started":"2022-05-26T13:11:26.67124Z","shell.execute_reply":"2022-05-26T13:11:28.022369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#THis code is to check if the video is corrupted or not..\n#If the video is corrupted delete the video.\nimport glob\nimport torch\nimport torchvision\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data.dataset import Dataset\nimport os\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\n#import face_recognition\n#Check if the file is corrupted or not\ndef validate_video(vid_path,train_transforms):\n    transform = train_transforms\n    count = 20\n    video_path = vid_path\n    frames = []\n    a = int(100/count)\n    first_frame = np.random.randint(0,a)\n    temp_video = video_path.split('/')[-1]\n    for i,frame in enumerate(frame_extract(video_path)):\n        frames.append(transform(frame))\n        if(len(frames) == count):\n            break\n    frames = torch.stack(frames)\n    frames = frames[:count]\n    return frames\n#extract a from from video\ndef frame_extract(path):\n    vidObj = cv2.VideoCapture(path) \n    success = 1\n    while success:\n        success, image = vidObj.read()\n        if success:\n            yield image\n\nim_size = 112\nmean = [0.485, 0.456, 0.406]\nstd = [0.229, 0.224, 0.225]\n\ntrain_transforms = transforms.Compose([\n                                        transforms.ToPILImage(),\n                                        transforms.Resize((im_size,im_size)),\n                                        transforms.ToTensor(),\n                                        transforms.Normalize(mean,std)])\nvideo_fil =  glob.glob('../input/deepfake-dataset/Modified Dataset/Modified Dataset/*.mp4')\nprint(\"Total no of videos :\" , len(video_fil))\n#print(video_fil)\ncount = 0;\ncorrupt_list = []\nfor i in label_data.loc[:, 'video_path']:\n    try:\n        validate_video(i,train_transforms)\n    except:\n        count+=1\n        corrupt_list.append(i)\n        print(\"Number of video processed: \" , count ,\" Remaining : \" , (len(label_data) - count))\n        print(\"Corrupted video is : \" , i)\n        continue\nprint((len(label_data) - count))\nlabel_data = label_data[~label_data['video_path'].isin(corrupt_list)]","metadata":{"id":"QZ22Sj8d0JoT","execution":{"iopub.status.busy":"2022-05-26T13:11:28.02587Z","iopub.execute_input":"2022-05-26T13:11:28.02647Z","iopub.status.idle":"2022-05-26T13:12:26.058038Z","shell.execute_reply.started":"2022-05-26T13:11:28.026416Z","shell.execute_reply":"2022-05-26T13:12:26.05693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#to load preprocessod video to memory\nimport json\nimport glob\nimport numpy as np\nimport cv2\nimport copy\nimport random\n# video_files =  glob.glob('/content/drive/My Drive/Celeb_fake_face_only/*.mp4')\n# video_files += glob.glob('/content/drive/My Drive/Celeb_real_face_only/*.mp4')\n# video_files += glob.glob('/content/drive/My Drive/DFDC_FAKE_Face_only_data/*.mp4')\n# video_files += glob.glob('/content/drive/My Drive/DFDC_REAL_Face_only_data/*.mp4')\n# video_files += glob.glob('/content/drive/My Drive/FF_Face_only_data/*.mp4')\nvideo_files = glob.glob('../input/deepfake-dataset/Modified Dataset/Modified Dataset/*.mp4')\nrandom.shuffle(video_files)\n\nframe_count = []\nfor video_file in video_files:\n    cap = cv2.VideoCapture(video_file)\n    if(int(cap.get(cv2.CAP_PROP_FRAME_COUNT))<100):\n        video_files.remove(video_file)\n        continue\n    frame_count.append(int(cap.get(cv2.CAP_PROP_FRAME_COUNT)))\nprint(\"frames are \" , frame_count)\nprint(\"Total no of video: \" , len(frame_count))\nprint('Average frame per video:',np.mean(frame_count))","metadata":{"id":"CEIygy8uDFXc","execution":{"iopub.status.busy":"2022-05-26T13:12:26.064322Z","iopub.execute_input":"2022-05-26T13:12:26.064744Z","iopub.status.idle":"2022-05-26T13:12:30.050478Z","shell.execute_reply.started":"2022-05-26T13:12:26.06457Z","shell.execute_reply":"2022-05-26T13:12:30.049544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to reduce no. of frames of videos to save computation\n#!pip install face_recognition\nimport face_recognition\nfrom PIL import Image\n\nout_dir = \"./modified_videos/\"\n\ndef create_new_videos(paths, out_dir):\n    try:\n        if not os.path.exists(out_dir):\n            os.makedirs(out_dir)\n\n    # if not created then raise error\n    except OSError:\n        print ('Error: Creating directory of data')\n        return -1\n        \n    for path in paths:\n#         face_locs = []\n        frames = []\n        out_path = os.path.join(out_dir, path.split('/')[-1])\n        file_exists = glob(out_path)\n        if(len(file_exists) != 0):\n            print(\"File Already exists: \" , out_path)\n            continue\n            \n        out = cv2.VideoWriter(out_path, cv2.VideoWriter_fourcc('M','J','P','G'), 30, (112, 112))\n        for idx, image in enumerate(frame_extract(path)):\n\n            if(idx<150):\n                frames.append(image)\n                \n                if(len(frames)==4):\n                  #  for i in range(len(frames)):\n                    faces = face_recognition.batch_face_locations(frames)\n                    \n                    for i, face_location in enumerate(faces):\n                        face_image = 0\n                        if(len(face_location)!=0):\n                            top, right, bottom, left = face_location[0]\n                            face_image = frames[i][top:bottom, left:right, :]\n                            face_image = cv2.resize(face_image, (112, 112))\n                            \n                        try:\n                            out.write(face_image)\n                        except:\n                            pass\n                    frames = []\n            \n        try:\n            del top,right,bottom,left\n        except:\n            pass\n        out.release()\n           \n            #             cv2.rectangle(image, (x, y), (x+w, y+h), (0, 255, 0), 2)\n            #             cv2.imshow(\"Faces found\", image)\n\n            #         print(\"A face is located at pixel location Top: {}, Left: {}, Bottom: {}, Right: {}\".format(top, left, bottom, right))\n\n                # You can access the actual face itself like this:\n#             if(len(face_location)!=0):\n#                 top, right, bottom, left = face_location[0]\n#                 face_image = frames[i][top:bottom, left:right]\n                \n\n#                 image_arr.append(face_image)\n#     #             pil_image = Image.fromarray(face_image)\n#     #             pil_image.show()\n# #                 plt.imshow(face_image)\n# #                 plt.show()\n\n# #         print( face_recognition.face_locations(frames[0]))\n#         for i in range(len(image_arr)):\n#             out.write(image_arr[i])\n#             print('creating frame: ', i)\n\n#         out.release()\n                \ncreate_new_videos(labeled_files_train[2500:], out_dir)","metadata":{"execution":{"iopub.status.busy":"2022-05-22T09:17:01.955426Z","iopub.execute_input":"2022-05-22T09:17:01.955672Z","iopub.status.idle":"2022-05-22T09:17:04.803499Z","shell.execute_reply.started":"2022-05-22T09:17:01.955624Z","shell.execute_reply":"2022-05-22T09:17:04.80233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load the video name and labels from csv\nimport torch\nimport torchvision\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data.dataset import Dataset\nimport os\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\n#import face_recognition\nclass video_dataset(Dataset):\n    def __init__(self,video_names,label_data,sequence_length = 60,transform = None):\n        self.video_names = video_names\n        self.labels = label_data\n        self.transform = transform\n        self.count = sequence_length\n    def __len__(self):\n        return len(self.video_names)\n    def __getitem__(self,idx):\n        video_path = self.video_names[idx]\n        frames = []\n        a = int(150/self.count)\n        first_frame = np.random.randint(0,a)\n        temp_video = video_path.split('/')[-1]\n        #print(temp_video)\n        label = label_data[label_data.video == temp_video].labels\n        label = label.iloc[0]\n        for i,frame in enumerate(self.frame_extract(video_path)):\n            frames.append(self.transform(frame))\n            if(len(frames) == self.count):\n                break\n        frames = torch.stack(frames)\n        frames = frames[:self.count]\n        #print(\"length:\" , len(frames), \"label\",label)\n        return frames,label\n    def frame_extract(self,path):\n        vidObj = cv2.VideoCapture(path) \n        success = 1\n        while success:\n            success, image = vidObj.read()\n            if success:\n                yield image\n#plot the image\ndef im_plot(tensor):\n    image = tensor.cpu().numpy().transpose(1,2,0)\n    b,g,r = cv2.split(image)\n    image = cv2.merge((r,g,b))\n    image = image*[0.22803, 0.22145, 0.216989] +  [0.43216, 0.394666, 0.37645]\n    image = image*255.0\n    plt.imshow(image.astype(int))\n    plt.show()","metadata":{"id":"OqGXNkqhDKZU","execution":{"iopub.status.busy":"2022-05-26T13:12:30.052401Z","iopub.execute_input":"2022-05-26T13:12:30.052728Z","iopub.status.idle":"2022-05-26T13:12:30.071272Z","shell.execute_reply.started":"2022-05-26T13:12:30.052681Z","shell.execute_reply":"2022-05-26T13:12:30.070343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#count the number of fake and real videos\ndef number_of_real_and_fake_videos(data_list):\n    header_list = [\"file\",\"label\"]\n    lab = label_data\n    fake = 0\n    real = 0\n    for i in data_list:\n        temp_video = i.split('/')[-1]\n        label = label_data[label_data.video == temp_video]\n        label = label.label.iloc[0]\n        if(label == 'FAKE'):\n            fake+=1\n        if(label == 'REAL'):\n            real+=1\n    return real,fake","metadata":{"id":"1leMozhXa5LF","execution":{"iopub.status.busy":"2022-05-26T13:12:30.07304Z","iopub.execute_input":"2022-05-26T13:12:30.074501Z","iopub.status.idle":"2022-05-26T13:12:30.086712Z","shell.execute_reply.started":"2022-05-26T13:12:30.074446Z","shell.execute_reply":"2022-05-26T13:12:30.0858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load the labels and video in data loader\nimport random\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\nheader_list = [\"file\",\"label\"]\n#labels = pd.read_csv('../input/deepfake-dataset/New_DF.csv',names=header_list)\n#print(labels)\ntrain, valid = train_test_split(label_data, test_size = 0.2, random_state = 0)\ntrain.reset_index(drop=True, inplace=True)\nvalid.reset_index(drop=True, inplace=True)\ntrain_videos= train.loc[:, 'video_path']\ntrain_videos = list(train_videos)\nvalid_videos= valid.loc[:, 'video_path']\nvalid_videos = list(valid_videos)\nprint(\"train : \" , len(train_videos))\nprint(\"test : \" , len(valid_videos))\n# train_videos,valid_videos = train_test_split(data,test_size = 0.2)\n# print(train_videos)\n\nprint(\"TRAIN: \", \"Real:\",number_of_real_and_fake_videos(train_videos)[0],\" Fake:\",number_of_real_and_fake_videos(train_videos)[1])\nprint(\"TEST: \", \"Real:\",number_of_real_and_fake_videos(valid_videos)[0],\" Fake:\",number_of_real_and_fake_videos(valid_videos)[1])\n\n\nim_size = 112\nmean = [0.485, 0.456, 0.406]\nstd = [0.229, 0.224, 0.225]\n\ntrain_transforms = transforms.Compose([\n                                        transforms.ToPILImage(),\n                                        transforms.Resize((im_size,im_size)),\n                                        transforms.ToTensor(),\n                                        transforms.Normalize(mean,std)])\n\ntest_transforms = transforms.Compose([\n                                        transforms.ToPILImage(),\n                                        transforms.Resize((im_size,im_size)),\n                                        transforms.ToTensor(),\n                                        transforms.Normalize(mean,std)])\ntrain_data = video_dataset(train_videos,label_data,sequence_length = 10,transform = train_transforms)\n#print(train_data)\nval_data = video_dataset(valid_videos,label_data,sequence_length = 10,transform = train_transforms)\ntrain_loader = DataLoader(train_data,batch_size = 1,shuffle = True)\nvalid_loader = DataLoader(val_data,batch_size = 1,shuffle = True)\nimage,label = train_data[0]\nim_plot(image[0,:,:,:])","metadata":{"id":"sWMZn0YHDO2b","execution":{"iopub.status.busy":"2022-05-26T13:12:30.088997Z","iopub.execute_input":"2022-05-26T13:12:30.089732Z","iopub.status.idle":"2022-05-26T13:12:36.960339Z","shell.execute_reply.started":"2022-05-26T13:12:30.089282Z","shell.execute_reply":"2022-05-26T13:12:36.959506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Model with feature visualization\nfrom torch import nn\nfrom torchvision import models\nclass Model(nn.Module):\n    def __init__(self, num_classes,latent_dim= 2048, lstm_layers=1 , hidden_dim = 2048, bidirectional = False):\n        super(Model, self).__init__()\n        model = models.resnext50_32x4d(pretrained = True) #Residual Network CNN\n        self.model = nn.Sequential(*list(model.children())[:-2])\n        self.lstm = nn.LSTM(latent_dim,hidden_dim, lstm_layers,  bidirectional)\n        self.relu = nn.LeakyReLU()\n        self.dp = nn.Dropout(0.4)\n        self.linear1 = nn.Linear(2048,num_classes)\n        self.avgpool = nn.AdaptiveAvgPool2d(1)\n    def forward(self, x):\n        batch_size,seq_length, c, h, w = x.shape\n        x = x.view(batch_size * seq_length, c, h, w)\n        fmap = self.model(x)\n        x = self.avgpool(fmap)\n        x = x.view(batch_size,seq_length,2048)\n        x_lstm,_ = self.lstm(x,None)\n        return fmap,self.dp(self.linear1(torch.mean(x_lstm,dim = 1)))","metadata":{"id":"UtOXSqyBDRnD","execution":{"iopub.status.busy":"2022-05-26T13:12:36.966247Z","iopub.execute_input":"2022-05-26T13:12:36.968763Z","iopub.status.idle":"2022-05-26T13:12:36.988998Z","shell.execute_reply.started":"2022-05-26T13:12:36.96871Z","shell.execute_reply":"2022-05-26T13:12:36.987979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Model(2)\na,b = model(torch.from_numpy(np.empty((1,20,3,112,112))).type(torch.FloatTensor))\nmodel.load_state_dict(torch.load('../input/resnet50lstm-weights/model_84_acc_10_frames_final_data.pt',map_location=torch.device('cpu')))\n#model = torch.load('../input/resnet50lstm-weights/model_84_acc_10_frames_final_data.pt',map_location=torch.device('cpu'))","metadata":{"id":"WYNhn10tDV90","execution":{"iopub.status.busy":"2022-05-26T13:12:36.994126Z","iopub.execute_input":"2022-05-26T13:12:36.997179Z","iopub.status.idle":"2022-05-26T13:12:48.374716Z","shell.execute_reply.started":"2022-05-26T13:12:36.997125Z","shell.execute_reply":"2022-05-26T13:12:48.373936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Model(2).cuda()\na,b = model(torch.from_numpy(np.empty((1,20,3,112,112))).type(torch.cuda.FloatTensor))\nmodel.load_state_dict(torch.load('../input/resnet50lstm-weights/model_84_acc_10_frames_final_data.pt'))","metadata":{"execution":{"iopub.status.busy":"2022-05-26T13:12:48.376116Z","iopub.execute_input":"2022-05-26T13:12:48.376533Z","iopub.status.idle":"2022-05-26T13:12:54.273661Z","shell.execute_reply.started":"2022-05-26T13:12:48.376481Z","shell.execute_reply":"2022-05-26T13:12:54.272848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#import myutils\n#frames, v_len = myutils.get_frames('../input/deepfake-dataset/Modified Dataset/Modified Dataset/aahkgsltxw.mp4', n_frames\n#print(model('../input/deepfake-dataset/Modified Dataset/Modified Dataset/aahkgsltxw.mp4'))","metadata":{"execution":{"iopub.status.busy":"2022-05-22T09:44:40.052334Z","iopub.execute_input":"2022-05-22T09:44:40.052672Z","iopub.status.idle":"2022-05-22T09:44:40.056502Z","shell.execute_reply.started":"2022-05-22T09:44:40.052614Z","shell.execute_reply":"2022-05-22T09:44:40.055659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with torch.no_grad():\n    out = model(imgs_tensor.to(device)).cpu()\n    print(out.shape)\n    pred = torch.argmax(out).item()\n    print(pred)","metadata":{"execution":{"iopub.status.busy":"2022-05-26T14:46:08.449449Z","iopub.execute_input":"2022-05-26T14:46:08.449748Z","iopub.status.idle":"2022-05-26T14:46:08.467656Z","shell.execute_reply.started":"2022-05-26T14:46:08.449698Z","shell.execute_reply":"2022-05-26T14:46:08.466169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch.autograd import Variable\nimport time\nimport os\nimport sys\nimport os\ndef train_epoch(epoch, num_epochs, data_loader, model, criterion, optimizer):\n    model.train()\n    losses = AverageMeter()\n    accuracies = AverageMeter()\n    t = []\n    for i, (inputs, targets) in enumerate(data_loader):\n        if torch.cuda.is_available():\n            targets = targets.type(torch.cuda.LongTensor)\n            inputs = inputs.cuda()\n        _,outputs = model(inputs)\n        loss  = criterion(outputs,targets.type(torch.cuda.LongTensor))\n        acc = calculate_accuracy(outputs, targets.type(torch.cuda.LongTensor))\n        losses.update(loss.item(), inputs.size(0))\n        accuracies.update(acc, inputs.size(0))\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        sys.stdout.write(\n                \"\\r[Epoch %d/%d] [Batch %d / %d] [Loss: %f, Acc: %.2f%%]\"\n                % (\n                    epoch,\n                    num_epochs,\n                    i,\n                    len(data_loader),\n                    losses.avg,\n                    accuracies.avg))\n    torch.save(model.state_dict(),'./checkpoint.pt')\n    return losses.avg,accuracies.avg\ndef test(epoch,model, data_loader ,criterion):\n    print('Testing')\n    model.eval()\n    losses = AverageMeter()\n    accuracies = AverageMeter()\n    pred = []\n    true = []\n    count = 0\n    with torch.no_grad():\n        for i, (inputs, targets) in enumerate(data_loader):\n            if torch.cuda.is_available():\n                targets = targets.cuda().type(torch.cuda.FloatTensor)\n                inputs = inputs.cuda()\n            _,outputs = model(inputs)\n            loss = torch.mean(criterion(outputs, targets.type(torch.cuda.LongTensor)))\n            acc = calculate_accuracy(outputs,targets.type(torch.cuda.LongTensor))\n            _,p = torch.max(outputs,1) \n            true += (targets.type(torch.cuda.LongTensor)).detach().cpu().numpy().reshape(len(targets)).tolist()\n            pred += p.detach().cpu().numpy().reshape(len(p)).tolist()\n            losses.update(loss.item(), inputs.size(0))\n            accuracies.update(acc, inputs.size(0))\n    return true,pred,losses.avg,accuracies.avg\nclass AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n    def __init__(self):\n        self.reset()\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\ndef calculate_accuracy(outputs, targets):\n    batch_size = targets.size(0)\n\n    _, pred = outputs.topk(1, 1, True)\n    pred = pred.t()\n    correct = pred.eq(targets.view(1, -1))\n    n_correct_elems = correct.float().sum().item()\n    return 100* n_correct_elems / batch_size","metadata":{"id":"FKheLUWBDaNN","execution":{"iopub.status.busy":"2022-05-26T13:12:54.2752Z","iopub.execute_input":"2022-05-26T13:12:54.275666Z","iopub.status.idle":"2022-05-26T13:12:54.301567Z","shell.execute_reply.started":"2022-05-26T13:12:54.275468Z","shell.execute_reply":"2022-05-26T13:12:54.300819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sn\n#Output confusion matrix\ndef print_confusion_matrix(y_true, y_pred):\n    cm = confusion_matrix(y_true, y_pred)\n    print('True positive = ', cm[0][0])\n    print('False positive = ', cm[0][1])\n    print('False negative = ', cm[1][0])\n    print('True negative = ', cm[1][1])\n    print('\\n')\n    df_cm = pd.DataFrame(cm, range(2), range(2))\n    sn.set(font_scale=1.4) # for label size\n    sn.heatmap(df_cm, annot=True, annot_kws={\"size\": 16}) # font size\n    plt.ylabel('Actual label', size = 20)\n    plt.xlabel('Predicted label', size = 20)\n    plt.xticks(np.arange(2), ['Fake', 'Real'], size = 16)\n    plt.yticks(np.arange(2), ['Fake', 'Real'], size = 16)\n    plt.ylim([2, 0])\n    plt.show()\n    calculated_acc = (cm[0][0]+cm[1][1])/(cm[0][0]+cm[0][1]+cm[1][0]+ cm[1][1])\n    print(\"Calculated Accuracy\",calculated_acc*100)","metadata":{"id":"b8WneBZNfysN","execution":{"iopub.status.busy":"2022-05-26T13:12:54.30391Z","iopub.execute_input":"2022-05-26T13:12:54.304562Z","iopub.status.idle":"2022-05-26T13:12:54.409684Z","shell.execute_reply.started":"2022-05-26T13:12:54.304507Z","shell.execute_reply":"2022-05-26T13:12:54.408919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_loss(train_loss_avg,test_loss_avg,num_epochs):\n    loss_train = train_loss_avg\n    loss_val = test_loss_avg\n    print(num_epochs)\n    epochs = range(1,num_epochs+1)\n    plt.plot(epochs, loss_train, 'g', label='Training loss')\n    plt.plot(epochs, loss_val, 'b', label='validation loss')\n    plt.title('Training and Validation loss')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.show()\ndef plot_accuracy(train_accuracy,test_accuracy,num_epochs):\n    loss_train = train_accuracy\n    loss_val = test_accuracy\n    epochs = range(1,num_epochs+1)\n    plt.plot(epochs, loss_train, 'g', label='Training accuracy')\n    plt.plot(epochs, loss_val, 'b', label='validation accuracy')\n    plt.title('Training and Validation accuracy')\n    plt.xlabel('Epochs')\n    plt.ylabel('Accuracy')\n    plt.legend()\n    plt.show()","metadata":{"id":"fExJLjt2AtV9","execution":{"iopub.status.busy":"2022-05-26T13:12:54.410958Z","iopub.execute_input":"2022-05-26T13:12:54.41125Z","iopub.status.idle":"2022-05-26T13:12:54.420336Z","shell.execute_reply.started":"2022-05-26T13:12:54.411205Z","shell.execute_reply":"2022-05-26T13:12:54.419598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\n#learning rate\nlr = 1e-5#0.001\n#number of epochs \nnum_epochs = 20\n\noptimizer = torch.optim.Adam(model.parameters(), lr= lr,weight_decay = 1e-5)\n\n#class_weights = torch.from_numpy(np.asarray([1,15])).type(torch.FloatTensor).cuda()\n#criterion = nn.CrossEntropyLoss(weight = class_weights).cuda()\ncriterion = nn.CrossEntropyLoss().cuda()\ntrain_loss_avg =[]\ntrain_accuracy = []\ntest_loss_avg = []\ntest_accuracy = []\nfor epoch in range(1,num_epochs+1):\n    l, acc = train_epoch(epoch,num_epochs,train_loader,model,criterion,optimizer)\n    train_loss_avg.append(l)\n    train_accuracy.append(acc)\n    true,pred,tl,t_acc = test(epoch,model,valid_loader,criterion)\n    test_loss_avg.append(tl)\n    test_accuracy.append(t_acc)\n#plot_loss(train_loss_avg,test_loss_avg,len(train_loss_avg))\n#plot_accuracy(train_accuracy,test_accuracy,len(train_accuracy))\n#print(confusion_matrix(true,pred))\n#print_confusion_matrix(true,pred)","metadata":{"id":"rUe1XrYnDdit","execution":{"iopub.status.busy":"2022-05-26T13:12:54.421685Z","iopub.execute_input":"2022-05-26T13:12:54.422295Z","iopub.status.idle":"2022-05-26T14:23:44.367178Z","shell.execute_reply.started":"2022-05-26T13:12:54.422231Z","shell.execute_reply":"2022-05-26T14:23:44.366448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install face_recognition\nimport torch\nimport torchvision\nfrom torchvision import transforms, models\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data.dataset import Dataset\nimport os\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\nimport face_recognition\nfrom torch.autograd import Variable\nimport time\nimport sys\nfrom torch import nn\nimport json\nimport glob\nimport copy\nfrom torchvision import models\nimport shutil\nfrom PIL import Image as pImage\nimport time\n# from django.conf import settings\n# from .forms import VideoUploadForm\n\nindex_template_name = 'index.html'\npredict_template_name = 'predict.html'\n\nim_size = 112\nmean=[0.485, 0.456, 0.406]\nstd=[0.229, 0.224, 0.225]\nsm = nn.Softmax()\ninv_normalize =  transforms.Normalize(mean=-1*np.divide(mean,std),std=np.divide([1,1,1],std))\n\ntrain_transforms = transforms.Compose([\n                                        transforms.ToPILImage(),\n                                        transforms.Resize((im_size,im_size)),\n                                        transforms.ToTensor(),\n                                        transforms.Normalize(mean,std)])\n\nclass Model(nn.Module):\n\n    def __init__(self, num_classes,latent_dim= 2048, lstm_layers=1 , hidden_dim = 2048, bidirectional = False):\n        super(Model, self).__init__()\n        model = models.resnext50_32x4d(pretrained = True)\n        self.model = nn.Sequential(*list(model.children())[:-2])\n        self.lstm = nn.LSTM(latent_dim,hidden_dim, lstm_layers,  bidirectional)\n        self.relu = nn.LeakyReLU()\n        self.dp = nn.Dropout(0.4)\n        self.linear1 = nn.Linear(2048,num_classes)\n        self.avgpool = nn.AdaptiveAvgPool2d(1)\n\n    def forward(self, x):\n        batch_size,seq_length, c, h, w = x.shape\n        x = x.view(batch_size * seq_length, c, h, w)\n        fmap = self.model(x)\n        x = self.avgpool(fmap)\n        x = x.view(batch_size,seq_length,2048)\n        x_lstm,_ = self.lstm(x,None)\n        return fmap,self.dp(self.linear1(x_lstm[:,-1,:]))\n\n\nclass validation_dataset(Dataset):\n    def __init__(self,video_names,sequence_length=60,transform = None):\n        self.video_names = video_names\n        self.transform = transform\n        self.count = sequence_length\n\n    def __len__(self):\n        return len(self.video_names)\n\n    def __getitem__(self,idx):\n        video_path = self.video_names[idx]\n        frames = []\n        a = int(100/self.count)\n        first_frame = np.random.randint(0,a)\n        for i,frame in enumerate(self.frame_extract(video_path)):\n            #if(i % a == first_frame):\n            faces = face_recognition.face_locations(frame)\n            try:\n                top,right,bottom,left = faces[0]\n                frame = frame[top:bottom,left:right,:]\n            except:\n                pass\n            frames.append(self.transform(frame))\n            if(len(frames) == self.count):\n                break\n        \"\"\"\n        for i,frame in enumerate(self.frame_extract(video_path)):\n            if(i % a == first_frame):\n                frames.append(self.transform(frame))\n        \"\"\"        \n        # if(len(frames)<self.count):\n        #   for i in range(self.count-len(frames)):\n        #         frames.append(self.transform(frame))\n        #print(\"no of frames\", self.count)\n        frames = torch.stack(frames)\n        frames = frames[:self.count]\n        return frames.unsqueeze(0)\n    \n    def frame_extract(self,path):\n        vidObj = cv2.VideoCapture(path) \n        success = 1\n        while success:\n            success, image = vidObj.read()\n            if success:\n                yield image\n\ndef im_convert(tensor, video_file_name):\n    \"\"\" Display a tensor as an image. \"\"\"\n    image = tensor.to(\"cpu\").clone().detach()\n    image = image.squeeze()\n    image = inv_normalize(image)\n    image = image.numpy()\n    image = image.transpose(1,2,0)\n    image = image.clip(0, 1)\n    # This image is not used\n    cv2.imwrite(os.path.join('./','uploaded_images', video_file_name+'_convert_2.png'),image*255)\n    return image\n\ndef im_plot(tensor):\n    image = tensor.cpu().numpy().transpose(1,2,0)\n    b,g,r = cv2.split(image)\n    image = cv2.merge((r,g,b))\n    image = image*[0.22803, 0.22145, 0.216989] +  [0.43216, 0.394666, 0.37645]\n    image = image*255.0\n    plt.imshow(image.astype(int))\n    plt.show()\n\n\ndef predict(model,img,path = './', video_file_name=\"\"):\n    fmap,logits = model(img.to('cuda'))\n    img = im_convert(img[:,-1,:,:,:], video_file_name)\n    params = list(model.parameters())\n    weight_softmax = model.linear1.weight.detach().cpu().numpy()\n    logits = sm(logits)\n    _,prediction = torch.max(logits,1)\n    confidence = logits[:,int(prediction.item())].item()*100\n    print('confidence of prediction:',logits[:,int(prediction.item())].item()*100)  \n    return [int(prediction.item()),confidence]\n\ndef plot_heat_map(i, model, img, path = './', video_file_name=''):\n    fmap,logits = model(img.to('cuda'))\n    params = list(model.parameters())\n    weight_softmax = model.linear1.weight.detach().cpu().numpy()\n    logits = sm(logits)\n    _,prediction = torch.max(logits,1)\n    idx = np.argmax(logits.detach().cpu().numpy())\n    bz, nc, h, w = fmap.shape\n    #out = np.dot(fmap[-1].detach().cpu().numpy().reshape((nc, h*w)).T,weight_softmax[idx,:].T)\n    out = np.dot(fmap[i].detach().cpu().numpy().reshape((nc, h*w)).T,weight_softmax[idx,:].T)\n    predict = out.reshape(h,w)\n    predict = predict - np.min(predict)\n    predict_img = predict / np.max(predict)\n    predict_img = np.uint8(255*predict_img)\n    out = cv2.resize(predict_img, (im_size,im_size))\n    heatmap = cv2.applyColorMap(out, cv2.COLORMAP_JET)\n    img = im_convert(img[:,-1,:,:,:], video_file_name)\n    result = heatmap * 0.5 + img*0.8*255\n    # Saving heatmap - Start\n    heatmap_name = video_file_name+\"_heatmap_\"+str(i)+\".png\"\n    image_name = os.path.join('./','uploaded_images', heatmap_name)\n    cv2.imwrite(image_name,result)\n    # Saving heatmap - End\n    result1 = heatmap * 0.5/255 + img*0.8\n    r,g,b = cv2.split(result1)\n    result1 = cv2.merge((r,g,b))\n    return image_name","metadata":{"execution":{"iopub.status.busy":"2022-05-29T06:12:41.532650Z","iopub.execute_input":"2022-05-29T06:12:41.533249Z","iopub.status.idle":"2022-05-29T06:12:50.773458Z","shell.execute_reply.started":"2022-05-29T06:12:41.532968Z","shell.execute_reply":"2022-05-29T06:12:50.772649Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Requirement already satisfied: face_recognition in /opt/conda/lib/python3.6/site-packages (1.3.0)\nRequirement already satisfied: dlib>=19.7 in /opt/conda/lib/python3.6/site-packages (from face_recognition) (19.24.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from face_recognition) (1.17.4)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.6/site-packages (from face_recognition) (5.4.1)\nRequirement already satisfied: face-recognition-models>=0.3.0 in /opt/conda/lib/python3.6/site-packages (from face_recognition) (0.3.0)\nRequirement already satisfied: Click>=6.0 in /opt/conda/lib/python3.6/site-packages (from face_recognition) (7.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"def prediction(video_file):\n        out_dir = \"./uploaded_images/\"\n\n\n        try:\n            if not os.path.exists(out_dir):\n                os.makedirs(out_dir)\n\n        # if not created then raise error\n        except OSError:\n            print ('Error: Creating directory of data')\n            \n        sequence_length = 10\n        #video_file = '../input/deepfake-detection-challenge/test_videos/aayfryxljh.mp4'\n        path_to_videos = [video_file]\n        video_file_name = video_file.split('\\\\')[-1]\n        video_file_name_only = video_file_name.split('.')[0]\n        video_dataset = validation_dataset(path_to_videos, sequence_length=sequence_length,transform= train_transforms)\n        model = Model(2).cuda()\n#         model_name = os.path.join(settings.PROJECT_DIR,'models', get_accurate_model(sequence_length))\n#         models_location = os.path.join(settings.PROJECT_DIR,'models')\n        path_to_model = '../input/resnet50lstm-weights/model_acc_93.pt'\n        model.load_state_dict(torch.load(path_to_model))\n        model.eval()\n        start_time = time.time()\n        # Start: Displaying preprocessing images\n        print(\"<=== | Started Videos Splitting | ===>\")\n        preprocessed_images = []\n        faces_cropped_images = []\n        cap = cv2.VideoCapture(video_file)\n\n        frames = []\n        while(cap.isOpened()):\n            ret, frame = cap.read()\n            if ret==True:\n                frames.append(frame)\n                if cv2.waitKey(1) & 0xFF == ord('q'):\n                    break\n            else:\n                break\n        cap.release()\n\n        for i in range(1, sequence_length+1):\n            frame = frames[i]\n            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n            img = pImage.fromarray(image, 'RGB')\n            image_name = video_file_name_only+\"_preprocessed_\"+str(i)+'.png'\n            image_path = os.path.join('./','uploaded_images', image_name)\n            img.save(image_path)\n            preprocessed_images.append(image_name)\n        print(\"<=== | Videos Splitting Done | ===>\")\n        print(\"--- %s seconds ---\" % (time.time() - start_time))\n        # End: Displaying preprocessing images\n\n\n        # Start: Displaying Faces Cropped Images\n        print(\"<=== | Started Face Cropping Each Frame | ===>\")\n        padding = 40\n        faces_found = 0\n        for i in range(1, sequence_length+1):\n            frame = frames[i]\n            #fig, ax = plt.subplots(1,1, figsize=(5, 5))\n            face_locations = face_recognition.face_locations(frame)\n            if len(face_locations) == 0:\n                continue\n            top, right, bottom, left = face_locations[0]\n            frame_face = frame[top-padding:bottom+padding, left-padding:right+padding]\n            image = cv2.cvtColor(frame_face, cv2.COLOR_BGR2RGB)\n\n            img = pImage.fromarray(image, 'RGB')\n            image_name = video_file_name_only+\"_cropped_faces_\"+str(i)+'.png'\n            image_path = os.path.join('./','uploaded_images', video_file_name_only+\"_cropped_faces_\"+str(i)+'.png')\n            img.save(image_path)\n            faces_found = faces_found + 1\n            faces_cropped_images.append(image_name)\n        print(\"<=== | Face Cropping Each Frame Done | ===>\")\n        print(\"--- %s seconds ---\" % (time.time() - start_time))\n\n        # No face is detected\n        if faces_found == 0:\n            print('no faces')\n\n        # End: Displaying Faces Cropped Images\n        try:\n            heatmap_images = []\n            for i in range(0, len(path_to_videos)):\n                output = \"\"\n                print(\"<=== | Started Predicition | ===>\")\n                prediction = predict(model, video_dataset[i], './', video_file_name_only)\n                confidence = round(prediction[1], 1)\n                print(\"<=== |  Predicition Done | ===>\")\n                # print(\"<=== | Heat map creation started | ===>\")\n                # for j in range(0, sequence_length):\n                #     heatmap_images.append(plot_heat_map(j, model, video_dataset[i], './', video_file_name_only))\n                if prediction[0] == 1:\n                    output = \"REAL\"\n                else:\n                    output = \"FAKE\"\n                print(\"Prediction : \" , prediction[0],\"==\",output ,\"Confidence : \" , confidence)\n                print(\"--- %s seconds ---\" % (time.time() - start_time))\n        except:\n            pass","metadata":{"execution":{"iopub.status.busy":"2022-05-29T06:13:36.316214Z","iopub.execute_input":"2022-05-29T06:13:36.316542Z","iopub.status.idle":"2022-05-29T06:13:36.340323Z","shell.execute_reply.started":"2022-05-29T06:13:36.316469Z","shell.execute_reply":"2022-05-29T06:13:36.339597Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"path='../input/deepfake-detection-challenge/test_videos/aassnaulhq.mp4'\nprediction(path)","metadata":{"execution":{"iopub.status.busy":"2022-05-29T06:13:40.875015Z","iopub.execute_input":"2022-05-29T06:13:40.875324Z","iopub.status.idle":"2022-05-29T06:14:28.715913Z","shell.execute_reply.started":"2022-05-29T06:13:40.875272Z","shell.execute_reply":"2022-05-29T06:14:28.715235Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/resnext50_32x4d-7cdf4587.pth\" to /root/.cache/torch/checkpoints/resnext50_32x4d-7cdf4587.pth\n100%|██████████| 95.8M/95.8M [00:07<00:00, 13.1MB/s]\n","output_type":"stream"},{"name":"stdout","text":"<=== | Started Videos Splitting | ===>\n<=== | Videos Splitting Done | ===>\n--- 6.924931287765503 seconds ---\n<=== | Started Face Cropping Each Frame | ===>\n<=== | Face Cropping Each Frame Done | ===>\n--- 19.04708766937256 seconds ---\n<=== | Started Predicition | ===>\nconfidence of prediction: 99.68844056129456\n<=== |  Predicition Done | ===>\nPrediction :  1 == REAL Confidence :  99.7\n--- 31.635884046554565 seconds ---\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:136: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Extracting labels from metadata.csv\nlabel_data = pd.read_csv(\"../input/deepfake-dataset/New_DF.csv\")\nlabel_data.drop(axis=1, columns=['Unnamed: 0'], inplace=True)\npath = '../input/deepfake-dataset/Modified Dataset/Modified Dataset/'\nlabel_data['video_path'] = path+label_data['video']\nfrom sklearn import preprocessing\nle = preprocessing.LabelEncoder()\nlabels = le.fit_transform(label_data['label'])\nlabel_data['labels'] = labels\nlabel_data","metadata":{"execution":{"iopub.status.busy":"2022-05-26T14:31:22.482301Z","iopub.execute_input":"2022-05-26T14:31:22.482608Z","iopub.status.idle":"2022-05-26T14:31:22.515982Z","shell.execute_reply.started":"2022-05-26T14:31:22.482546Z","shell.execute_reply":"2022-05-26T14:31:22.515083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}